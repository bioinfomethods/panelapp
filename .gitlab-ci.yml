include:
  - local: ".gitlab-ci/envs.yml"
  - template: Code-Quality.gitlab-ci.yml

variables:
  DOCKER_TERRAFORM: hashicorp/terraform:1.4.6
  DOCKER_PYTHON: python:3.8.17-alpine3.18
  DOCKER_POSTGRES: postgres:14.7
  DOCKER_IN_DOCKER: docker:23.0.3-dind
  FF_GITLAB_REGISTRY_HELPER_IMAGE: 1
  PANELAPP_TF_DIR: "panelapp-infra/terraform/panelapp/"
  AWS_REGION: eu-west-2
  AWS_DEFAULT_REGION: eu-west-2
  AWS_ASSUME_ROLE: "arn:aws:iam::${AWS_ACCOUNT_ID}:role/CIDeploypanelapp"
  ECR_ACCOUNT_ID: 577192787797
  ECR_REGISTRY: "${ECR_ACCOUNT_ID}.dkr.ecr.eu-west-2.amazonaws.com"
  ECR_ASSUME_ROLE: "arn:aws:iam::${ECR_ACCOUNT_ID}:role/CIDeploypanelapp"
  ECR_REPOSITORY: panelapp
  WORKFLOW_NAME: "Deploying app: $version, infra: $infra_version -> $env_name"
  TAGGER_REGISTRY: $ECR_ACCOUNT_ID
  TAGGER_REPOSITORY: $ECR_REPOSITORY
  TAGGER_ENVIRONMENTS: "build,dev,test,e2e,uat,prod"
  #  TAGGER_DEBUG: 'yes'
  CI_DEBUG_TRACE: "false"
  #  TF_LOG: DEBUG
  UI_TEST_IMAGE_TAG: "$CI_REGISTRY_IMAGE/ui-test:$CI_COMMIT_SHORT_SHA"
  env_name:
    description: "Environment to deploy to"
    value: dev
    options:
      - dev
      - test
      - e2e
      - "### regulated environments"
      - uat
      - prod
  version:
    description: "Version to deploy"
    value: 3.8.0
  infra_version:
    description: "Infra version (tag or branch) to use"
    value: 2.3.0
  snapshot_arn:
    description: "ARN of snapshot to use for the database; DESTRUCTIVE action"
    value: ""

stages:
  - test_build
  - test
  - image_build
  - plan
  - apply

default:
  tags:
    - pool_name:panelapp_docker

.deploy:
  image:
    name: $DOCKER_TERRAFORM
    entrypoint: ["/bin/sh", "-c"]
  before_script:
    - apk --no-cache add --update python3 curl jq aws-cli py3-boto3

.tests:
  image: $DOCKER_PYTHON
  stage: test
  services:
    - name: $DOCKER_IN_DOCKER
      command:
        - "--registry-mirror"
        - "https://docker.artifactory.aws.gel.ac"
  rules:
    - if: $TEST

.tag:
  image: $DOCKER_IN_DOCKER
  before_script:
    - apk add aws-cli jq py3-boto3
    - source scripts/assume_role.sh
  variables:
    AWS_ASSUME_ROLE: $ECR_ASSUME_ROLE

code_quality:
  extends: .tests
  artifacts:
    paths: [gl-code-quality-report.json]
  rules:
    - if: $TEST

# Code checks
## run unit and integration tests
code_test:
  extends: .tests
  coverage: /^TOTAL.+?(\d+\%)$/
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
  services:
    - name: $DOCKER_POSTGRES
      alias: db-postgres
  variables:
    POSTGRES_DB: panelapp
    POSTGRES_USER: panelapp
    POSTGRES_PASSWORD: secret
    DATABASE_URL: postgres://panelapp:secret@db-postgres:5432/panelapp
    DJANGO_SETTINGS_MODULE: panelapp.settings.test
    DJANGO_LOG_LEVEL: INFO
  before_script:
    - apk add --no-cache postgresql-libs git curl jpeg-dev zlib-dev gcc musl-dev curl-dev postgresql-dev build-base linux-headers libffi-dev
    - python -m venv .venv
    - source .venv/bin/activate
    - pip install .[tests]
    - pip install pytest-runner
  script:
    - pytest --cov-report term --cov-report xml

# Build docker image for UI testing purposes
ui_test_image_build:
  image: $DOCKER_IN_DOCKER
  services:
    - name: $DOCKER_IN_DOCKER
      command:
        - "--registry-mirror"
        - "https://docker.artifactory.aws.gel.ac"
  stage: test_build
  before_script:
    - apk add aws-cli jq py3-boto3
    - source scripts/assume_role.sh
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_JOB_TOKEN $CI_REGISTRY
    - docker build -f ./docker/Dockerfile -t "$UI_TEST_IMAGE_TAG" --target test .
    - docker push "$UI_TEST_IMAGE_TAG"
  rules:
    - if: $TEST

ui_test:
  image: mcr.microsoft.com/playwright:v1.40.0-jammy
  stage: test
  dependencies:
    - ui_test_image_build
  artifacts:
    paths:
      - test-results/
      - playwright-report/
    when: always
  services:
    # Alternative? Use docker-compose inline for all services + testing
    # https://forum.gitlab.com/t/run-project-with-docker-compose/35975
    # - name: $DOCKER_IN_DOCKER
    #   command:
    #     - "--registry-mirror"
    #     - "https://docker.artifactory.aws.gel.ac"
    - name: $DOCKER_POSTGRES
      alias: db-postgres
    - name: $UI_TEST_IMAGE_TAG
      alias: panelapp
      command:
        - /bin/sh
        - "-c"
        - "python manage.py migrate && python manage.py loaddata /app/data.json && python manage.py runserver 0.0.0.0:8080"
  variables:
    # Allows inter-service communication: https://stackoverflow.com/questions/48041101/gitlab-ci-cross-service-communication
    FF_NETWORK_PER_BUILD: 1
    # Variables for services defined here
    # Service-specific env config is only available from Gitlab runner version 14.5, we use version 13.9.0
    # postgres
    POSTGRES_DB: panelapp
    POSTGRES_USER: panelapp
    POSTGRES_PASSWORD: secret
    # panelapp
    PYTHONUNBUFFERED: "1"
    DJANGO_LOG_LEVEL: DEBUG
    DJANGO_SETTINGS_MODULE: panelapp.settings.test
    DATABASE_URL: postgres://panelapp:secret@db-postgres:5432/panelapp
    BASE_URL: http://panelapp:8080
    ALLOWED_HOSTS: panelapp
  script:
    - corepack enable
    - yarn set version 3.6.3
    - yarn install
    - yarn playwright test
  rules:
    - if: $TEST

## Linting, formatting, etc
import_sorting:
  extends: .tests
  before_script:
    - python -m venv .venv
    - source .venv/bin/activate
    - pip install isort==5.0.5
  script:
    - isort . -c

formatter:
  extends: .tests
  before_script:
    - python -m venv .venv
    - source .venv/bin/activate
    - pip install black==22.3.0
  script:
    - black . --check

trivy:
  stage: test
  image:
    name: aquasec/trivy:0.42.1
    entrypoint: [""]
  script:
    - trivy fs --exit-code 0 --cache-dir .trivycache/ --severity MEDIUM,HIGH,CRITICAL --ignore-unfixed --format template --template "@/contrib/junit.tpl" -o trivy.junit.xml .
    - trivy fs --exit-code 1 --cache-dir .trivycache/ --severity CRITICAL --ignore-unfixed --no-progress .
    - if [[ -s .trivyignore ]]; then echo ".trivyignore is not empty" ; cat .trivyignore ; exit 2 ; fi
  allow_failure:
    exit_codes:
      - 2 # warn when .trivyignore exists and is not empty
  rules:
    - if: $TEST
      allow_failure: true
      when: never
  artifacts:
    when: always
    reports:
      junit: "trivy.junit.xml"
    paths:
      - "trivy.junit.xml"

# Build docker image
image_build:
  image: $DOCKER_IN_DOCKER
  services:
    - name: $DOCKER_IN_DOCKER
      command:
        - "--registry-mirror"
        - "https://docker.artifactory.aws.gel.ac"
  stage: image_build
  before_script:
    - apk add aws-cli jq py3-boto3
    - source scripts/assume_role.sh
  script:
    - build_tag="ci-$CI_JOB_ID"
    - '(scripts/tagger validate-version "$CI_COMMIT_TAG" && ! scripts/tagger tag-exists "$CI_COMMIT_TAG") || scripts/tagger freestyle-tag "$CI_COMMIT_TAG"'
    - aws ecr get-login-password --region eu-west-2 | docker login --username AWS --password-stdin "$ECR_REGISTRY"
    - if scripts/tagger validate-version "$CI_COMMIT_TAG"; then echo "${CI_COMMIT_TAG}" > ./VERSION; fi
    - docker build -f ./docker/Dockerfile -t "$ECR_REGISTRY/$ECR_REPOSITORY:$build_tag" --target prod .
    - docker push "$ECR_REGISTRY/$ECR_REPOSITORY:$build_tag"
    - scripts/tagger add-env-tags --env=build --add-latest=1 "$build_tag"
    - scripts/tagger add-tag "$build_tag" "$CI_COMMIT_TAG"
  variables:
    AWS_ASSUME_ROLE: $ECR_ASSUME_ROLE
  rules:
    - if: $CI_COMMIT_TAG != null

# Deploy all the things

check_version:
  stage: plan
  extends: .tag
  script:
    - "printf '###\n### checking application version: %s\n###\n' \"$version\""
    - scripts/tagger tag-exists "$version"
    - test "$env_name" != e2e || scripts/tagger validate-version "$version"
    - test "$env_name" != uat || scripts/tagger validate-version --no-alpha --no-beta "$version"
    - test "$env_name" != prod || scripts/tagger validate-version --no-alpha --no-beta --no-rc "$version"
    - "printf '###\n### checking infrastructure version: %s\n###\n' \"$infra_version\""
    - test "$env_name" != e2e || scripts/tagger validate-version "$infra_version"
    - test "$env_name" != uat || scripts/tagger validate-version --no-alpha --no-beta "$infra_version"
    - test "$env_name" != prod || scripts/tagger validate-version --no-alpha --no-beta --no-rc "$infra_version"
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_COMMIT_TAG != null

scan:
  stage: plan
  dependencies:
    - image_build
  environment:
    name: $env_name
  image: 877059142592.dkr.ecr.eu-west-2.amazonaws.com/ce_infra_cicd:v1.0.6
  before_script:
    - apk add aws-cli jq py3-boto3
    - source scripts/assume_role.sh
  script:
    - aws ecr describe-image-scan-findings --repository-name "$ECR_REPOSITORY" --image-id imageTag="$version" | tee "inspector_findings_$version.json"
    - count_findings () { jq ".imageScanFindings.findingSeverityCounts.$1 // 0" "inspector_findings_$version.json"; }
    - |
      if [ "$(count_findings CRITICAL)" -gt 0 ]; then
        echo -e "\e[91mThe image "$ECR_REPOSITORY:$version" has $(count_findings CRITICAL) CRITICAL findings.\e[0m"
        exit 101
      elif [ "$(count_findings HIGH)" -gt 0 ]; then
        echo -e "\e[93mThe image "$ECR_REPOSITORY:$version" has $(count_findings HIGH) HIGH findings.\e[0m"
        exit 102
      else
        echo -e "\e[92mAll good!\e[0m"
      fi
  variables:
    AWS_ASSUME_ROLE: $ECR_ASSUME_ROLE
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_COMMIT_TAG != null
  allow_failure:
    exit_codes:
      - 101
      - 102
  artifacts:
    paths:
      - inspector_findings_$version.json
    when: on_failure

plan:
  stage: plan
  extends: .deploy
  environment:
    name: $env_name
  dependencies:
    - image_build
  script:
    - printf '###\n### terraform plan\n###\n'
    - git clone --single-branch --branch "$infra_version" https://$TF_CODE_REPO_USER:$TF_CODE_REPO_TOKEN@gitlab.com/genomicsengland/panelapp/panelapp-infra.git
    - cd "$PANELAPP_TF_DIR"
    - terraform init -backend-config="../config/$env_name/panelapp-backend.conf"
    - |
      if [ -n "$snapshot_arn" ]; then
        (
          cd "$CI_PROJECT_DIR"
          source scripts/assume_role.sh
          aws rds describe-db-cluster-snapshots --filter="Name=db-cluster-snapshot-id,Values=$snapshot_arn" --query DBClusterSnapshots[].Status --output text | grep -q available || { echo "snapshot $snapshot_arn does not exists" >&2; exit 1; }
        )
        terraform plan -var-file="../config/$env_name/terraform.tfvars" \
          -var image_tag="$version" -var infra_tag="$infra_version" \
          -var=restore_from_snapshot=true -var=snapshot_identifier=$snapshot_arn \
          -replace=module.aurora.aws_rds_cluster.aurora_cluster \
          -out=plan.tfplan
      else
        terraform plan -var-file="../config/$env_name/terraform.tfvars" -var image_tag="$version" -var infra_tag="$infra_version" -out=plan.tfplan
      fi
    - "printf '### Environment:   %s\n### App version:   %s\n### Infra version: %s\n### Snapshot:      %s\n' \"$env_name\" \"$version\" \"$infra_version\" \"$snapshot_arn\""
  rules:
    - if: $CI_COMMIT_TAG != null && $snapshot_arn == ""
    - if: $CI_PIPELINE_SOURCE == "web"
  artifacts:
    paths:
      - panelapp-infra/terraform/
      - panelapp-infra/*.py

apply:
  stage: apply
  extends: .deploy
  environment:
    name: $env_name
  dependencies:
    - plan
    - scan
  script:
    - |
      if [ -n "$snapshot_arn" ]; then
        (
          source "$CI_PROJECT_DIR/scripts/assume_role.sh"
          cluster_name="aurora-$env_name"
          cluster_status="$(aws rds describe-db-clusters --query "DBClusters[?DBClusterIdentifier == '$cluster_name'].Status" --output text)"
          final_snapshot_id="$cluster_name-final-$(date -u +'%Y-%m-%d-%H-%M')"
          if [ "$cluster_status" = "available" ]; then
            echo "Create snapshot before delete: $final_snapshot_id"
            aws rds create-db-cluster-snapshot --db-cluster-snapshot-identifier "$final_snapshot_id" --db-cluster-identifier "$cluster_name"
            while true; do
              status=$(aws rds describe-db-cluster-snapshots --query "DBClusterSnapshots[?DBClusterSnapshotIdentifier == '$final_snapshot_id'].Status" --output text)
              aws rds describe-db-cluster-snapshots --query "DBClusterSnapshots[?DBClusterSnapshotIdentifier == '$final_snapshot_id'].[@.DBClusterSnapshotIdentifier, @.Status]" --output text
              if [ "$status" = "available" ]; then
                break
              fi
              sleep 15
            done
          else
            echo "Can not create snapshot: cluster is not available"
          fi
        )
      fi
    - cd "$PANELAPP_TF_DIR"
    - terraform apply plan.tfplan
    - cd -
    - source scripts/assume_role.sh
    - time aws ecs wait services-stable --cluster panelapp-cluster-$env_name --services panelapp-web-panelapp-$env_name panelapp-worker-panelapp-$env_name panelapp-worker-beat-panelapp-$env_name
    - panelapp-infra/run-ecs-task.py panelapp-migrate-panelapp-$env_name panelapp-collectstatic-panelapp-$env_name
    - sleep 30
    - test "$env_name" = "prod" || panelapp-infra/run-ecs-task.py panelapp-datacleanup-$env_name
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_COMMIT_TAG != null
      when: manual

add_tags:
  stage: apply
  extends: .tag
  needs:
    - apply
  script:
    - scripts/tagger add-env-tags --env="$env_name" --add-latest=3 "$version"
    - scripts/tagger --verbose list-tags
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_COMMIT_TAG != null
